+ mkdir -p /mnt/sharedfs/nebius-demoday-test/slurm-logs
+ source training/env.sh
++ VENV=/mnt/sharedfs/nebius-demoday-test/.venv
++ '[' -d /mnt/sharedfs/nebius-demoday-test/.venv ']'
++ source /mnt/sharedfs/nebius-demoday-test/.venv/bin/activate
+++ deactivate nondestructive
+++ '[' -n '' ']'
+++ '[' -n '' ']'
+++ hash -r
+++ '[' -n '' ']'
+++ unset VIRTUAL_ENV
+++ unset VIRTUAL_ENV_PROMPT
+++ '[' '!' nondestructive = nondestructive ']'
+++ '[' linux-gnu = cygwin ']'
+++ '[' linux-gnu = msys ']'
+++ export VIRTUAL_ENV=/mnt/sharedfs/nebius-demoday-test/.venv
+++ VIRTUAL_ENV=/mnt/sharedfs/nebius-demoday-test/.venv
+++ _OLD_VIRTUAL_PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
+++ PATH=/mnt/sharedfs/nebius-demoday-test/.venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
+++ export PATH
+++ '[' -n '' ']'
+++ '[' -z '' ']'
+++ _OLD_VIRTUAL_PS1=
+++ PS1='(.venv) '
+++ export PS1
+++ VIRTUAL_ENV_PROMPT='(.venv) '
+++ export VIRTUAL_ENV_PROMPT
+++ hash -r
++ set -euo pipefail
++ REPO_ROOT=/mnt/sharedfs/nebius-demoday-test
++ export RUNS_ROOT=/mnt/sharedfs/nebius-demoday-test/results/training
++ RUNS_ROOT=/mnt/sharedfs/nebius-demoday-test/results/training
++ export HF_HOME=/mnt/sharedfs/nebius-demoday-test/.cache/huggingface
++ HF_HOME=/mnt/sharedfs/nebius-demoday-test/.cache/huggingface
++ mkdir -p /mnt/sharedfs/nebius-demoday-test/results/training /mnt/sharedfs/nebius-demoday-test/.cache/huggingface
++ echo REPO_ROOT=/mnt/sharedfs/nebius-demoday-test
++ echo RUNS_ROOT=/mnt/sharedfs/nebius-demoday-test/results/training
++ echo HF_HOME=/mnt/sharedfs/nebius-demoday-test/.cache/huggingface
+ source /mnt/sharedfs/nebius-demoday-test/.venv/bin/activate
++ deactivate nondestructive
++ '[' -n /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin ']'
++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
++ export PATH
++ unset _OLD_VIRTUAL_PATH
++ '[' -n '' ']'
++ hash -r
++ '[' -n '' ']'
++ unset VIRTUAL_ENV
++ unset VIRTUAL_ENV_PROMPT
++ '[' '!' nondestructive = nondestructive ']'
++ '[' linux-gnu = cygwin ']'
++ '[' linux-gnu = msys ']'
++ export VIRTUAL_ENV=/mnt/sharedfs/nebius-demoday-test/.venv
++ VIRTUAL_ENV=/mnt/sharedfs/nebius-demoday-test/.venv
++ _OLD_VIRTUAL_PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
++ PATH=/mnt/sharedfs/nebius-demoday-test/.venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
++ export PATH
++ '[' -n '' ']'
++ '[' -z '' ']'
++ _OLD_VIRTUAL_PS1='(.venv) '
++ PS1='(.venv) (.venv) '
++ export PS1
++ VIRTUAL_ENV_PROMPT='(.venv) '
++ export VIRTUAL_ENV_PROMPT
++ hash -r
+ export NCCL_DEBUG=INFO
+ NCCL_DEBUG=INFO
+ export TORCH_DISTRIBUTED_DEBUG=DETAIL
+ TORCH_DISTRIBUTED_DEBUG=DETAIL
+ export NCCL_IB_DISABLE=0
+ NCCL_IB_DISABLE=0
+ export 'NCCL_SOCKET_IFNAME=^lo,docker0'
+ NCCL_SOCKET_IFNAME='^lo,docker0'
++ hostname
+ echo HOST=worker0
+ echo CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
+ nvidia-smi -L
+ torchrun --nproc_per_node=8 training/src/train_sft_ddp_min.py --run_name sft_ddp_1node_smoke --max_steps 10 --seq_len 512
W0125 00:38:29.142000 157079 torch/distributed/run.py:793] 
W0125 00:38:29.142000 157079 torch/distributed/run.py:793] *****************************************
W0125 00:38:29.142000 157079 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0125 00:38:29.142000 157079 torch/distributed/run.py:793] *****************************************
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.11it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.06s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.12it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.10it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.07it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.08it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.68s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.10it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.85it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.15it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.18it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.11it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.17it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.29it/s]
Map:   0%|          | 0/2048 [00:00<?, ? examples/s]Map:   0%|          | 0/2048 [00:00<?, ? examples/s]Map:   0%|          | 0/2048 [00:00<?, ? examples/s]Map:   0%|          | 0/2048 [00:00<?, ? examples/s]Map:   0%|          | 0/2048 [00:00<?, ? examples/s]Map:  14%|█▍        | 291/2048 [00:00<00:00, 2884.92 examples/s]Map:  13%|█▎        | 267/2048 [00:00<00:00, 2651.35 examples/s]Map:   0%|          | 0/2048 [00:00<?, ? examples/s]Map:  15%|█▌        | 310/2048 [00:00<00:00, 3070.72 examples/s]Map:   0%|          | 0/2048 [00:00<?, ? examples/s]Map:  14%|█▎        | 277/2048 [00:00<00:00, 2749.97 examples/s]Map:  14%|█▍        | 285/2048 [00:00<00:00, 2823.58 examples/s]Map:  27%|██▋       | 563/2048 [00:00<00:00, 2825.90 examples/s]Map:  15%|█▌        | 313/2048 [00:00<00:00, 3101.91 examples/s]Map:  15%|█▌        | 308/2048 [00:00<00:00, 3057.47 examples/s]Map:  29%|██▉       | 591/2048 [00:00<00:00, 2975.83 examples/s]Map:  35%|███▌      | 722/2048 [00:00<00:00, 2875.63 examples/s]Map:  37%|███▋      | 762/2048 [00:00<00:00, 3021.77 examples/s]Map:  30%|██▉       | 611/2048 [00:00<00:00, 3068.22 examples/s]Map:  42%|████▏     | 865/2048 [00:00<00:00, 2907.36 examples/s]Map:  31%|███       | 639/2048 [00:00<00:00, 3171.95 examples/s]Map:  30%|███       | 619/2048 [00:00<00:00, 3084.32 examples/s]Map:  44%|████▍     | 898/2048 [00:00<00:00, 3015.08 examples/s]Map:  45%|████▌     | 927/2048 [00:00<00:00, 3102.49 examples/s]Map:  57%|█████▋    | 1173/2048 [00:00<00:00, 2083.35 examples/s]Map:  57%|█████▋    | 1173/2048 [00:00<00:00, 2113.14 examples/s]Map:  57%|█████▋    | 1173/2048 [00:00<00:00, 1987.11 examples/s]Map:  49%|████▉     | 1000/2048 [00:00<00:00, 2060.80 examples/s]Map:  49%|████▉     | 1000/2048 [00:00<00:00, 1997.11 examples/s]Map:  66%|██████▌   | 1345/2048 [00:00<00:00, 2225.30 examples/s]Map:  65%|██████▌   | 1332/2048 [00:00<00:00, 2185.22 examples/s]Map:  72%|███████▏  | 1483/2048 [00:00<00:00, 2362.39 examples/s]Map:  73%|███████▎  | 1490/2048 [00:00<00:00, 2350.10 examples/s]Map:  73%|███████▎  | 1488/2048 [00:00<00:00, 2299.48 examples/s]Map:  66%|██████▌   | 1345/2048 [00:00<00:00, 2455.19 examples/s]Map:  65%|██████▍   | 1330/2048 [00:00<00:00, 2353.07 examples/s]Map:  80%|████████  | 1644/2048 [00:00<00:00, 2419.45 examples/s]Map:  80%|███████▉  | 1630/2048 [00:00<00:00, 2392.15 examples/s]Map:  86%|████████▌ | 1758/2048 [00:00<00:00, 2463.41 examples/s]Map:  86%|████████▌ | 1758/2048 [00:00<00:00, 2434.10 examples/s]Map:  80%|███████▉  | 1631/2048 [00:00<00:00, 2570.79 examples/s]Map:  79%|███████▉  | 1626/2048 [00:00<00:00, 2523.46 examples/s]Map:  93%|█████████▎| 1900/2048 [00:00<00:00, 2459.69 examples/s]Map:  95%|█████████▍| 1945/2048 [00:00<00:00, 2576.80 examples/s]Map:  94%|█████████▍| 1934/2048 [00:00<00:00, 2564.77 examples/s]Map:  94%|█████████▍| 1932/2048 [00:00<00:00, 2697.37 examples/s]Map:  94%|█████████▍| 1920/2048 [00:00<00:00, 2635.17 examples/s]Map: 100%|██████████| 2048/2048 [00:00<00:00, 1901.16 examples/s]Map: 100%|██████████| 2048/2048 [00:00<00:00, 2196.05 examples/s]
Map: 100%|██████████| 2048/2048 [00:00<00:00, 1869.93 examples/s]Map: 100%|██████████| 2048/2048 [00:00<00:00, 2138.44 examples/s]
Map: 100%|██████████| 2048/2048 [00:00<00:00, 2095.01 examples/s]
Map: 100%|██████████| 2048/2048 [00:00<00:00, 2082.24 examples/s]
Map: 100%|██████████| 2048/2048 [00:00<00:00, 2181.77 examples/s]
Map: 100%|██████████| 2048/2048 [00:00<00:00, 2202.40 examples/s]
Map: 100%|██████████| 2048/2048 [00:00<00:00, 2142.16 examples/s]
libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory
libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory
  0%|          | 0/10 [00:00<?, ?it/s][rank6]:[W125 00:39:15.537408600 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W125 00:39:15.538019443 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank4]:[W125 00:39:15.542860574 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W125 00:39:15.545353805 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank7]:[W125 00:39:15.549931377 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank5]:[W125 00:39:15.555013380 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W125 00:39:15.556097425 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W125 00:39:15.575914484 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mnt/sharedfs/nebius-demoday-test/training/src/train_sft_ddp_min.py", line 137, in <module>
[rank0]:     main()
[rank0]:   File "/mnt/sharedfs/nebius-demoday-test/training/src/train_sft_ddp_min.py", line 111, in main
[rank0]:     trainer.train()
[rank0]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2740, in _inner_training_loop
[rank0]:     self.optimizer.step()
[rank0]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/accelerate/optimizer.py", line 179, in step
[rank0]:     self.optimizer.step(closure)
[rank0]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
[rank0]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
[rank0]:     ret = func(self, *args, **kwargs)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 220, in step
[rank0]:     adamw(
[rank0]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 782, in adamw
[rank0]:     func(
[rank0]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
[rank0]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 100.50 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 74.44 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank4]: Traceback (most recent call last):
[rank4]:   File "/mnt/sharedfs/nebius-demoday-test/training/src/train_sft_ddp_min.py", line 137, in <module>
[rank4]:     main()
[rank4]:   File "/mnt/sharedfs/nebius-demoday-test/training/src/train_sft_ddp_min.py", line 111, in main
[rank4]:     trainer.train()
[rank4]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank4]:     return inner_training_loop(
[rank4]:            ^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2740, in _inner_training_loop
[rank4]:     self.optimizer.step()
[rank4]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/accelerate/optimizer.py", line 179, in step
[rank4]:     self.optimizer.step(closure)
[rank4]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
[rank4]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank4]:     out = func(*args, **kwargs)
[rank4]:           ^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
[rank4]:     ret = func(self, *args, **kwargs)
[rank4]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 220, in step
[rank4]:     adamw(
[rank4]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
[rank4]:     return func(*args, **kwargs)
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 782, in adamw
[rank4]:     func(
[rank4]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
[rank4]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank4]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 4 has a total capacity of 79.19 GiB of which 52.50 MiB is free. Including non-PyTorch memory, this process has 79.13 GiB memory in use. Of the allocated memory 74.44 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank2]: Traceback (most recent call last):
[rank2]:   File "/mnt/sharedfs/nebius-demoday-test/training/src/train_sft_ddp_min.py", line 137, in <module>
[rank2]:     main()
[rank2]:   File "/mnt/sharedfs/nebius-demoday-test/training/src/train_sft_ddp_min.py", line 111, in main
[rank2]:     trainer.train()
[rank2]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank2]:     return inner_training_loop(
[rank2]:            ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2740, in _inner_training_loop
[rank2]:     self.optimizer.step()
[rank2]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/accelerate/optimizer.py", line 179, in step
[rank2]:     self.optimizer.step(closure)
[rank2]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
[rank2]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank2]:     out = func(*args, **kwargs)
[rank2]:           ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
[rank2]:     ret = func(self, *args, **kwargs)
[rank2]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 220, in step
[rank2]:     adamw(
[rank2]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
[rank2]:     return func(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 782, in adamw
[rank2]:     func(
[rank2]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
[rank2]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank2]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 2 has a total capacity of 79.19 GiB of which 52.50 MiB is free. Including non-PyTorch memory, this process has 79.13 GiB memory in use. Of the allocated memory 74.44 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank7]: Traceback (most recent call last):
[rank7]:   File "/mnt/sharedfs/nebius-demoday-test/training/src/train_sft_ddp_min.py", line 137, in <module>
[rank7]:     main()
[rank7]:   File "/mnt/sharedfs/nebius-demoday-test/training/src/train_sft_ddp_min.py", line 111, in main
[rank7]:     trainer.train()
[rank7]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank7]:     return inner_training_loop(
[rank7]:            ^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2740, in _inner_training_loop
[rank7]:     self.optimizer.step()
[rank7]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/accelerate/optimizer.py", line 179, in step
[rank7]:     self.optimizer.step(closure)
[rank7]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
[rank7]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank7]:     out = func(*args, **kwargs)
[rank7]:           ^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
[rank7]:     ret = func(self, *args, **kwargs)
[rank7]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 220, in step
[rank7]:     adamw(
[rank7]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
[rank7]:     return func(*args, **kwargs)
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 782, in adamw
[rank7]:     func(
[rank7]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
[rank7]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank7]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 7 has a total capacity of 79.19 GiB of which 16.50 MiB is free. Including non-PyTorch memory, this process has 79.16 GiB memory in use. Of the allocated memory 74.70 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/mnt/sharedfs/nebius-demoday-test/training/src/train_sft_ddp_min.py", line 137, in <module>
[rank1]:     main()
[rank1]:   File "/mnt/sharedfs/nebius-demoday-test/training/src/train_sft_ddp_min.py", line 111, in main
[rank1]:     trainer.train()
[rank1]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank1]:     return inner_training_loop(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2740, in _inner_training_loop
[rank1]:     self.optimizer.step()
[rank1]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/accelerate/optimizer.py", line 179, in step
[rank1]:     self.optimizer.step(closure)
[rank1]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
[rank1]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank1]:     out = func(*args, **kwargs)
[rank1]:           ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
[rank1]:     ret = func(self, *args, **kwargs)
[rank1]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 220, in step
[rank1]:     adamw(
[rank1]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
[rank1]:     return func(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 782, in adamw
[rank1]:     func(
[rank1]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
[rank1]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank1]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 1 has a total capacity of 79.19 GiB of which 52.50 MiB is free. Including non-PyTorch memory, this process has 79.13 GiB memory in use. Of the allocated memory 74.44 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank6]: Traceback (most recent call last):
[rank6]:   File "/mnt/sharedfs/nebius-demoday-test/training/src/train_sft_ddp_min.py", line 137, in <module>
[rank6]:     main()
[rank6]:   File "/mnt/sharedfs/nebius-demoday-test/training/src/train_sft_ddp_min.py", line 111, in main
[rank6]:     trainer.train()
[rank6]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank6]:     return inner_training_loop(
[rank6]:            ^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2740, in _inner_training_loop
[rank6]:     self.optimizer.step()
[rank6]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/accelerate/optimizer.py", line 179, in step
[rank6]:     self.optimizer.step(closure)
[rank6]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
[rank6]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank6]:     out = func(*args, **kwargs)
[rank6]:           ^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
[rank6]:     ret = func(self, *args, **kwargs)
[rank6]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 220, in step
[rank6]:     adamw(
[rank6]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
[rank6]:     return func(*args, **kwargs)
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 782, in adamw
[rank6]:     func(
[rank6]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
[rank6]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank6]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 6 has a total capacity of 79.19 GiB of which 52.50 MiB is free. Including non-PyTorch memory, this process has 79.13 GiB memory in use. Of the allocated memory 74.44 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/mnt/sharedfs/nebius-demoday-test/training/src/train_sft_ddp_min.py", line 137, in <module>
[rank3]:     main()
[rank3]:   File "/mnt/sharedfs/nebius-demoday-test/training/src/train_sft_ddp_min.py", line 111, in main
[rank3]:     trainer.train()
[rank3]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank3]:     return inner_training_loop(
[rank3]:            ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2740, in _inner_training_loop
[rank3]:     self.optimizer.step()
[rank3]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/accelerate/optimizer.py", line 179, in step
[rank3]:     self.optimizer.step(closure)
[rank3]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
[rank3]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank3]:     out = func(*args, **kwargs)
[rank3]:           ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
[rank3]:     ret = func(self, *args, **kwargs)
[rank3]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 220, in step
[rank3]:     adamw(
[rank3]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
[rank3]:     return func(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 782, in adamw
[rank3]:     func(
[rank3]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
[rank3]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank3]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 3 has a total capacity of 79.19 GiB of which 52.50 MiB is free. Including non-PyTorch memory, this process has 79.13 GiB memory in use. Of the allocated memory 74.44 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank5]: Traceback (most recent call last):
[rank5]:   File "/mnt/sharedfs/nebius-demoday-test/training/src/train_sft_ddp_min.py", line 137, in <module>
[rank5]:     main()
[rank5]:   File "/mnt/sharedfs/nebius-demoday-test/training/src/train_sft_ddp_min.py", line 111, in main
[rank5]:     trainer.train()
[rank5]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2325, in train
[rank5]:     return inner_training_loop(
[rank5]:            ^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2740, in _inner_training_loop
[rank5]:     self.optimizer.step()
[rank5]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/accelerate/optimizer.py", line 179, in step
[rank5]:     self.optimizer.step(closure)
[rank5]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
[rank5]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank5]:     out = func(*args, **kwargs)
[rank5]:           ^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
[rank5]:     ret = func(self, *args, **kwargs)
[rank5]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 220, in step
[rank5]:     adamw(
[rank5]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
[rank5]:     return func(*args, **kwargs)
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 782, in adamw
[rank5]:     func(
[rank5]:   File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/optim/adamw.py", line 606, in _multi_tensor_adamw
[rank5]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank5]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 5 has a total capacity of 79.19 GiB of which 52.50 MiB is free. Including non-PyTorch memory, this process has 79.13 GiB memory in use. Of the allocated memory 74.44 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/10 [00:02<?, ?it/s]
[rank0]:[W125 00:39:17.083670930 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0125 00:39:19.538000 157079 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 157182 closing signal SIGTERM
E0125 00:39:19.704000 157079 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 157178) of binary: /mnt/sharedfs/nebius-demoday-test/.venv/bin/python
Traceback (most recent call last):
  File "/mnt/sharedfs/nebius-demoday-test/.venv/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/sharedfs/nebius-demoday-test/.venv/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
training/src/train_sft_ddp_min.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2026-01-25_00:39:19
  host      : worker0
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 157179)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2026-01-25_00:39:19
  host      : worker0
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 157181)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2026-01-25_00:39:19
  host      : worker0
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 157183)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2026-01-25_00:39:19
  host      : worker0
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 157184)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2026-01-25_00:39:19
  host      : worker0
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 157185)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2026-01-25_00:39:19
  host      : worker0
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 157186)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-01-25_00:39:19
  host      : worker0
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 157178)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
